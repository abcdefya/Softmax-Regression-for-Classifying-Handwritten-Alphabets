{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90884657-3c7d-4d4b-849d-ddffbbd31e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49039ca5-b3c1-422a-917c-36c21bd9cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15000 # Number of samples\n",
    "d = 16 # Features\n",
    "C = 26 # Class numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504abcdf-8bcd-42f0-99ee-f9a0ea9c161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get data from files\n",
    "def read_data(file_path):\n",
    "    X, Y = [], []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = line.strip().split(' ')\n",
    "            label = int(data[0])  \n",
    "            features = [float(feature.split(':')[1]) for feature in data[1:]]  \n",
    "            X.append(features)\n",
    "            Y.append(label)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d3dbe-e0b3-400b-a3a0-8166981a3181",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55974966-f310-4dc8-8345-327e0450de3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10500)\n",
      "(10500,)\n",
      "(16, 5000)\n",
      "(5000,)\n",
      "(16, 4500)\n",
      "(4500,)\n"
     ]
    }
   ],
   "source": [
    "file_train_path = \"./letter.scale.tr\" # Training dataset\n",
    "file_test_path = \"./letter.scale.t\" # Testing dataset\n",
    "file_val_path = \"./letter.scale.val\" # Validation dataset\n",
    "\n",
    "# Loading training dataset\n",
    "X_train, y_train = read_data(file_train_path)\n",
    "X_train = X_train.T\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Loading testing dataset\n",
    "X_test, y_test = read_data(file_test_path)\n",
    "X_test = X_test.T\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Loading validation dataset\n",
    "X_val, y_val= read_data(file_val_path)\n",
    "X_val = X_val.T\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f6fc8be-f6a4-4b0f-b8b0-e5819b32561c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d52b75-65d1-4931-9ca9-39b7b878791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin label is from 1 -> 26, so we need to subtract all by 1 to make sure that it goes from 0 -> 25\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "y_val = y_val - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7b35b82-2812-4818-b855-84963d4dfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y, num_classes):\n",
    "\n",
    "    Y = sparse.coo_matrix((np.ones_like(y), \n",
    "        (y, np.arange(len(y)))), shape=(num_classes, len(y))).toarray()\n",
    "    return Y \n",
    "\n",
    "\n",
    "def cross_entropy_loss(X, Y_hat, theta): # Y_hat is onehot form\n",
    "    Y = softmax_stable(np.dot(theta.T, X))\n",
    "    eps = 1e-15\n",
    "    Y_hat = np.clip(Y_hat, eps, 1 - eps)\n",
    "    loss = -np.sum(Y * np.log(Y_hat)) / Y.shape[1]\n",
    "    return loss \n",
    "\n",
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z)\n",
    "    A = e_Z / e_Z.sum(axis=0)\n",
    "    return A\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    A = e_Z / e_Z.sum(axis=0)\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca3ae7b7-fe87-40d7-b9c7-b4533b888f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    return accuracy\n",
    "\n",
    "def predict(theta, X):\n",
    "    A = softmax_stable(np.dot(theta.T, X))\n",
    "    return np.argmax(A, axis=0)\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414d68d-fafc-493a-a045-0d9f8b780495",
   "metadata": {},
   "source": [
    "#### Constant step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "905797dc-13ac-46c5-893e-980de7c7cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression(X, y, theta_init, eta, tol=1e-8, max_count=500000):\n",
    "    theta = theta_init\n",
    "    C = theta.shape[1]\n",
    "    Y = one_hot_encoding(y, C)\n",
    "    N = X.shape[1]\n",
    "    d = X.shape[0]\n",
    "    \n",
    "    count = 0\n",
    "    prev_theta = theta\n",
    "\n",
    "    while count < max_count:\n",
    "        # Shuffle data\n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in mix_id:\n",
    "            xi = X[:, i].reshape(d, 1)\n",
    "            yi = Y[:, i].reshape(C, 1)\n",
    "            ai = softmax(np.dot(theta.T, xi))\n",
    "            grad = xi.dot((ai - yi).T)\n",
    "            theta_new = theta - eta * grad\n",
    "            count += 1\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(theta_new - prev_theta) < tol:\n",
    "                return theta_new\n",
    "            \n",
    "            prev_theta = theta\n",
    "            theta = theta_new\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557b69e-29f4-409c-8f9c-8a1ec8e22fab",
   "metadata": {},
   "source": [
    "#### Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaaae2dd-95bd-4e9e-a934-28009a9b79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtraking_softmax(X_train, y_train, eta=1, m=0.5, alpha=0.5, batch_size=1024, epochs=256):\n",
    "    y_train1hot = one_hot_encoding(y_train, 26)\n",
    "    d, C = X_train.shape[0], y_train1hot.shape[0]\n",
    "    theta_init = np.random.randn(d, C)\n",
    "    theta = [theta_init]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, X_train.shape[1], batch_size):\n",
    "            # Get the batch\n",
    "            X_batch = X_train[:, i:i+batch_size]\n",
    "            y_batch = y_train1hot[:, i:i+batch_size]\n",
    "\n",
    "            # Compute the gradient over the batch\n",
    "            ai = softmax_stable(np.dot(theta[-1].T, X_batch))\n",
    "            grad = np.dot(X_batch, (ai - y_batch).T) / batch_size\n",
    "\n",
    "            # Update theta with backtracking line search\n",
    "            theta_new = theta[-1] - eta * grad\n",
    "            t_k = eta\n",
    "            j = 0\n",
    "            while (cross_entropy_loss(X_train, y_train1hot, theta_new) > cross_entropy_loss(X_train, y_train1hot, theta[-1]) - m * t_k * np.linalg.norm(grad) ** 2) and (j < 5):\n",
    "                t_k *= alpha\n",
    "                theta_new = theta[-1] - t_k * grad\n",
    "                j += 1\n",
    "\n",
    "            # Append the new theta to the list\n",
    "            theta.append(theta_new)\n",
    "            if np.linalg.norm(theta[-1] - theta[-2]) < 1e-8:\n",
    "                print(f\"Converged at epoch {epoch + 1}, iteration {i // batch_size + 1}\")\n",
    "                return theta[-1]\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {cross_entropy_loss(X_train, y_train1hot, theta[-1])}\")\n",
    "\n",
    "    # Final trained theta\n",
    "    return theta[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffa8f8-4cc8-4994-a91e-f0df37e57aa0",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11255377-9ff4-437e-bec4-b27870768526",
   "metadata": {},
   "source": [
    "## Constant step-size model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8d8ae81-c661-4cee-84bf-ac36872170c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta = 0.03\n",
    "theta_init = np.random.randn(d, C)\n",
    "theta = softmax_regression(X_train, y_train, theta_init, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b4f904-4578-4863-b865-68b08dce2226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.64431722  -4.1577112    1.56590038  -7.0136261   -3.99226107\n",
      "   -2.69107225   0.11563527  -4.34672849   3.87485429   0.39681656\n",
      "   -0.05464407   0.40487192   2.02555134  -1.27590916  -2.24071501\n",
      "    1.13816917   3.08357054  -1.95806934  -0.10107353   2.14360767\n",
      "    0.13745665   1.80319592   4.40600431  -0.78687476   3.65020917\n",
      "   -1.21479458]\n",
      " [ -2.69535216   2.61164918   1.46747786   5.31158617   1.37125232\n",
      "   -0.70923241   0.767559     1.17144991  -0.58689666  -3.28040068\n",
      "   -1.22775965   0.79344677  -0.28171723   0.11502387   2.10153485\n",
      "   -3.03091997  -2.36813771   1.01228289  -0.61013629   1.66164031\n",
      "    2.74274373   0.7637991   -2.06121909  -1.01795518  -2.33841057\n",
      "   -3.9693405 ]\n",
      " [  2.34817122  -2.03906091  -2.46862142  -2.08908117  -2.92717831\n",
      "    0.25921721   2.11992192   9.65190989 -11.5719076   -3.98175953\n",
      "    5.00953413  -3.69424656   6.6795214    6.60473442   2.2327069\n",
      "    0.11339362  -1.69991474  -0.27961241   1.62333231  -5.13491295\n",
      "   -1.03025006   1.33921553   1.10914801   5.66608636  -0.54408527\n",
      "   -3.89213757]\n",
      " [  4.78948798  -5.15343597  -1.52686649  -7.04415468  -1.72523581\n",
      "    1.34696475  -1.36101075  -5.3321498    5.85771756   8.63048557\n",
      "    0.2825501    1.30310037  -6.80671934  -1.51717259  -1.46181541\n",
      "    2.75950668   5.60693217  -2.28515131   1.77975382  -1.29949149\n",
      "   -4.22211196  -0.79334842  -2.06869288   4.13609661   3.15230845\n",
      "    4.83302586]\n",
      " [ -2.83146697   4.62232762  -1.21785716   5.38871995   3.39691595\n",
      "    2.19486732  -1.34480314   1.09801151   2.01453828  -1.94550833\n",
      "   -4.26361054   2.09399164  -0.0200317   -2.1802317   -1.58410016\n",
      "    2.69943173  -2.28945255   1.67206     -3.51881137   3.53307588\n",
      "    3.28048672  -1.0659089    1.16243624 -11.07200489  -1.92492472\n",
      "    1.14153878]\n",
      " [  4.83187701  -2.24614417  -1.99792114  -0.59445957  -2.91701807\n",
      "   -3.05437704  -1.90057212   0.14080229   3.2048389   10.20797813\n",
      "   -8.82737484  -3.72536385   0.85220162  -2.92283814   0.33556888\n",
      "   -1.5466798    6.00352781  -2.993407     2.45366299   3.74904611\n",
      "   -1.75067511  -2.05152062  -2.230476     0.09933798   1.39673182\n",
      "    5.8712607 ]\n",
      " [ -2.70190861  -1.69719168   3.72664307  -4.34148621   3.65219429\n",
      "    5.5687693   -3.47027111   3.20637976  -2.46688816   2.46621076\n",
      "    0.61270227 -11.70094818   0.90100102   4.62643231   0.65613788\n",
      "    0.53356416  -3.68229449   7.29526202  -3.03358653  12.3242502\n",
      "   -4.34465356   1.73394856   2.05830362  -1.28735485  -1.95603735\n",
      "   -1.93987959]\n",
      " [ -0.99121902   0.59317251   0.75320245   4.86668953  -2.90144136\n",
      "   -2.55242939   1.92321633   5.07594434  -4.68088058  -0.41961584\n",
      "   -4.75210835  -3.0695516    3.21056857   3.78841501   5.06638614\n",
      "    2.65601145   1.72015884   0.48247636  -0.69382277  -2.39294805\n",
      "    2.76242423   0.02174203   0.51539413  -9.22777258  -2.51413117\n",
      "   -3.05961513]\n",
      " [ -5.62924409   1.08913839   3.21673025   4.90862017   6.24098079\n",
      "   -1.46474299  -1.32594166  -4.27172689   3.07323974  -0.70461021\n",
      "    0.46469928   4.56209681  -3.41130359  -3.08964633  -2.63866777\n",
      "   -7.06492482  -4.60087957  -1.73902206   1.37892362   7.65671235\n",
      "    2.99216176  -7.46077283  -6.81988307   3.04291726  -1.94124143\n",
      "    9.04057727]\n",
      " [ -3.29224693   2.3840664    0.53114578   4.89899803   4.66578499\n",
      "    5.25962094  -1.9440526    1.1752879    0.73910728   5.62931807\n",
      "   -1.01859453  -3.47461356  -2.3595434   -0.86672164   0.61996532\n",
      "    4.1944977   -3.96151094  -1.16103018   2.40434127  -1.40479315\n",
      "   -6.06972521  -4.27046157  -3.00998324  -4.65451184  -3.43668395\n",
      "    7.11188962]\n",
      " [ -3.55594653  -2.44497004   1.04178153  -4.44609824  -1.56690769\n",
      "    0.09142331  -0.59519534  -3.26929681   3.35492172   2.19618237\n",
      "   -2.98315647  -0.42154911  -1.69138887  -4.74498746  -3.85925947\n",
      "   -0.73391655   0.0777789   -8.63955841   0.15208811  -0.99159712\n",
      "    6.03439633   9.36787123   5.02832026   1.80931174  11.9308258\n",
      "   -1.4788891 ]\n",
      " [  2.9274973   -8.48585795  13.05728376 -10.9393898    8.10728603\n",
      "   -5.36527425   6.37896492   0.79216728  -2.93917225  -0.68674956\n",
      "    6.68263113   3.79059398   4.37521186  -1.55686617   1.75667262\n",
      "  -12.4280623    4.59855984  -1.54956953  -0.96048352  -0.16828057\n",
      "    3.39744545  -5.51242982  -3.11974307  -0.73695154  -4.89265903\n",
      "    1.38832915]\n",
      " [  1.2558995   -4.18179509   1.53612922  -0.33827096  -2.74538613\n",
      "   -1.60124087  -3.91708335   0.95610082  -5.47898082  -5.26958829\n",
      "    3.89055188  -2.20303991  15.25609743   8.36068346  -1.03105395\n",
      "   -2.00920297  -4.90202897   0.21888994  -8.69045743  -3.72315913\n",
      "    4.47899007   3.26980571  12.57567919   0.87667612  -4.7346689\n",
      "   -7.14021869]\n",
      " [ -5.06681438  -0.59947852   4.11029569  -2.68954547   0.55211511\n",
      "    1.52556996   3.34240681  -0.33356986  -1.84612711  -7.34145954\n",
      "   -2.91611907   0.54983773  -8.47341463  -1.46946255   2.08064403\n",
      "    5.09437352   2.22046402  -6.34562835  -0.45527325   4.769718\n",
      "    0.70874759   3.24888546   1.74937848  -1.50402287   7.94074802\n",
      "   -0.32223322]\n",
      " [ -2.19253252   7.15402035   0.8457562   -0.11078361   8.39947702\n",
      "    3.5394431    2.61468253  -6.15508111   0.63963582   2.36794147\n",
      "   -0.21874385   1.30828283 -12.28081707 -11.09660335  -2.57573158\n",
      "    1.33861929   2.77144672   0.64415957   9.01639144  -0.14498257\n",
      "  -11.36245659  -1.67245972  -9.7428993    5.35980862   4.59664354\n",
      "   13.46118067]\n",
      " [ -3.91823235   5.21256188   5.95031512  -2.13953174   5.54765029\n",
      "    1.42678699   5.36046639   0.81878409   0.35649815  -4.54559981\n",
      "    8.18019059   1.71046175  -3.29050551  -4.48552163  -0.27876619\n",
      "    2.78861928   3.54773059   5.14799844  -1.47156947  -3.65375647\n",
      "   -6.96437252   1.45702305  -1.68512044   0.81995232  -4.07090284\n",
      "   -4.23799768]]\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27357343-4862-4024-be48-a8b12108f790",
   "metadata": {},
   "source": [
    "### Prediction on constant step-size model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0626fc40-e7b0-4481-ad82-5387f321ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for the first 20 data points in the training set:\n",
      " [21 11  8  2 21 10  9  3  7  6  9  4  3  2 21 18 24 11 23 19]\n",
      "Actual classes for the first 20 data points in the training set:\n",
      " [21 11  8  2 21 10  9  3  7  6  9  2  3  6 21  0 24 11 23 19]\n",
      "Accuracy for the training set: 75.67%\n",
      "Weighted Precision on the training set: 0.75\n",
      "Weighted Recall on the training set: 0.76\n",
      "Weighted F1 score on the training set: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Predict on the training set\n",
    "Y_train_pred = predict(theta, X_train)  \n",
    "print(\"Predicted classes for the first 20 data points in the training set:\\n\", Y_train_pred[:20])\n",
    "print(\"Actual classes for the first 20 data points in the training set:\\n\", y_train[:20])\n",
    "acc_train = accuracy_score(y_train.T, Y_train_pred)\n",
    "print(f\"Accuracy for the training set: {acc_train:.2f}%\")\n",
    "\n",
    "# Calculate Precision, Recall, and F1 score on the training set\n",
    "precision, recall, f1 = calculate_f1_score(y_train, Y_train_pred)\n",
    "\n",
    "print(f'Weighted Precision on the training set: {precision:.2f}')\n",
    "print(f'Weighted Recall on the training set: {recall:.2f}')\n",
    "print(f'Weighted F1 score on the training set: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b058d278-b489-41c7-bcc1-bbc65a5080f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for the first 20 data points in the test set:\n",
      " [16 17  5  5 19 19 16  9  2 14 21  8 14 17  4 16  5 23  5  4]\n",
      "Actual classes for the first 20 data points in the test set:\n",
      " [16 17  5  5 19 19  6 25  2 14 21  8  7  3  4 16 15 23  5  4]\n",
      "Accuracy for the test set: 75.12%\n",
      "Weighted Precision on the test set: 0.75\n",
      "Weighted Recall on the test set: 0.75\n",
      "Weighted F1 score on the test set: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "Y_test_pred = predict(theta, X_test)\n",
    "print(\"Predicted classes for the first 20 data points in the test set:\\n\", Y_test_pred[:20])\n",
    "print(\"Actual classes for the first 20 data points in the test set:\\n\", y_test[:20])\n",
    "\n",
    "acc_test = accuracy_score(y_test.T, Y_test_pred)\n",
    "print(f\"Accuracy for the test set: {acc_test:.2f}%\")\n",
    "\n",
    "# Calculate Precision, Recall, and F1 score on the test set\n",
    "test_precision, test_recall, test_f1 = calculate_f1_score(y_test, Y_test_pred)\n",
    "\n",
    "print(f'Weighted Precision on the test set: {test_precision:.2f}')\n",
    "print(f'Weighted Recall on the test set: {test_recall:.2f}')\n",
    "print(f'Weighted F1 score on the test set: {test_f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d06e9b35-7c95-4e0c-bace-d8649a670e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for the first 20 data points in the validation set:\n",
      " [13  5  2 10  9 11  0  1 17  4 25  1 15 11 13 17  5 12  4 19]\n",
      "Actual classes for the first 20 data points in the validation set:\n",
      " [13  5  6 17  9 11  0  1 18 25 18  1 15 11 13 17  5 12  4 19]\n",
      "Accuracy for the validation set: 75.40%\n",
      "Weighted Precision on the validation set: 0.75\n",
      "Weighted Recall on the validation set: 0.75\n",
      "Weighted F1 score on the validation set: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set\n",
    "Y_val_pred = predict(theta, X_val)\n",
    "print(\"Predicted classes for the first 20 data points in the validation set:\\n\", Y_val_pred[:20])\n",
    "print(\"Actual classes for the first 20 data points in the validation set:\\n\", y_val[:20])\n",
    "\n",
    "acc_val = accuracy_score(y_val.T, Y_val_pred)\n",
    "print(f\"Accuracy for the validation set: {acc_val:.2f}%\")\n",
    "\n",
    "# Calculate Precision, Recall, and F1 score on the validation set\n",
    "val_precision, val_recall, val_f1 = calculate_f1_score(y_val, Y_val_pred)\n",
    "\n",
    "print(f'Weighted Precision on the validation set: {val_precision:.2f}')\n",
    "print(f'Weighted Recall on the validation set: {val_recall:.2f}')\n",
    "print(f'Weighted F1 score on the validation set: {val_f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106ef6e-78a3-4d32-90f0-151e4757cb9c",
   "metadata": {},
   "source": [
    "## Backtracking (Armijo step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f88dcc46-835f-4104-bb4f-0d564dcf9b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 33.14231549145548\n",
      "Epoch 2, Loss: 33.115271150662984\n",
      "Epoch 3, Loss: 33.046731851486705\n",
      "Epoch 4, Loss: 32.7040069160328\n",
      "Epoch 5, Loss: 32.43478574341474\n",
      "Epoch 6, Loss: 32.1807675998398\n",
      "Epoch 7, Loss: 31.921270556515417\n",
      "Epoch 8, Loss: 31.6556046072193\n",
      "Epoch 9, Loss: 31.38439749795307\n",
      "Epoch 10, Loss: 31.108175989858907\n",
      "Epoch 11, Loss: 30.828196023612723\n",
      "Epoch 12, Loss: 30.546467013653555\n",
      "Epoch 13, Loss: 30.265409342264647\n",
      "Epoch 14, Loss: 29.987457683052863\n",
      "Epoch 15, Loss: 29.71472931937366\n",
      "Epoch 16, Loss: 29.44882492250892\n",
      "Epoch 17, Loss: 29.190778150648963\n",
      "Epoch 18, Loss: 28.941117134010874\n",
      "Epoch 19, Loss: 28.699979908152734\n",
      "Epoch 20, Loss: 28.46723663037537\n",
      "Epoch 21, Loss: 28.2425931307137\n",
      "Epoch 22, Loss: 28.025667948829188\n",
      "Epoch 23, Loss: 27.81604462013765\n",
      "Epoch 24, Loss: 27.613304504623365\n",
      "Epoch 25, Loss: 27.417045745434745\n",
      "Epoch 26, Loss: 27.22689298005967\n",
      "Epoch 27, Loss: 27.04250121577018\n",
      "Epoch 28, Loss: 26.86355622288597\n",
      "Epoch 29, Loss: 26.68977299487393\n",
      "Epoch 30, Loss: 26.520893255627072\n",
      "Epoch 31, Loss: 26.35668261018112\n",
      "Epoch 32, Loss: 26.196927684215947\n",
      "Epoch 33, Loss: 26.041433438484834\n",
      "Epoch 34, Loss: 25.890020746304472\n",
      "Epoch 35, Loss: 25.742524264050836\n",
      "Epoch 36, Loss: 25.59879059172017\n",
      "Epoch 37, Loss: 25.458676703400283\n",
      "Epoch 38, Loss: 25.322048619705928\n",
      "Epoch 39, Loss: 25.18878029189586\n",
      "Epoch 40, Loss: 25.058752668052723\n",
      "Epoch 41, Loss: 24.931852913891728\n",
      "Epoch 42, Loss: 24.807973763605787\n",
      "Epoch 43, Loss: 24.687012979171286\n",
      "Epoch 44, Loss: 24.568872899470506\n",
      "Epoch 45, Loss: 24.453460063307073\n",
      "Epoch 46, Loss: 24.34068489284555\n",
      "Epoch 47, Loss: 24.230461426181293\n",
      "Epoch 48, Loss: 24.12270708964678\n",
      "Epoch 49, Loss: 24.01734250210453\n",
      "Epoch 50, Loss: 23.91429130488249\n",
      "Epoch 51, Loss: 23.81348001220196\n",
      "Epoch 52, Loss: 23.71483787795057\n",
      "Epoch 53, Loss: 23.618296775489508\n",
      "Epoch 54, Loss: 23.523791087873715\n",
      "Epoch 55, Loss: 23.431257606429345\n",
      "Epoch 56, Loss: 23.340635436090324\n",
      "Epoch 57, Loss: 23.251865906263593\n",
      "Epoch 58, Loss: 23.164892486285435\n",
      "Epoch 59, Loss: 23.07966070476109\n",
      "Epoch 60, Loss: 22.99611807225936\n",
      "Epoch 61, Loss: 22.914214006971857\n",
      "Epoch 62, Loss: 22.83389976305114\n",
      "Epoch 63, Loss: 22.755128361420628\n",
      "Epoch 64, Loss: 22.6778545229068\n",
      "Epoch 65, Loss: 22.602034603585654\n",
      "Epoch 66, Loss: 22.52762653226498\n",
      "Epoch 67, Loss: 22.454589750043315\n",
      "Epoch 68, Loss: 22.38288515189978\n",
      "Epoch 69, Loss: 22.312475030276072\n",
      "Epoch 70, Loss: 22.24332302061641\n",
      "Epoch 71, Loss: 22.175394048832217\n",
      "Epoch 72, Loss: 22.10865428065825\n",
      "Epoch 73, Loss: 22.043071072865256\n",
      "Epoch 74, Loss: 21.981890141106255\n",
      "Epoch 75, Loss: 21.92472575963314\n",
      "Epoch 76, Loss: 21.870272853049983\n",
      "Epoch 77, Loss: 21.8177193685223\n",
      "Epoch 78, Loss: 21.76659649950947\n",
      "Epoch 79, Loss: 21.71662063391361\n",
      "Epoch 80, Loss: 21.667612970070227\n",
      "Epoch 81, Loss: 21.61945625114056\n",
      "Epoch 82, Loss: 21.572070459678123\n",
      "Epoch 83, Loss: 21.525398706438544\n",
      "Epoch 84, Loss: 21.479398819053383\n",
      "Epoch 85, Loss: 21.43403821556669\n",
      "Epoch 86, Loss: 21.389290717327068\n",
      "Epoch 87, Loss: 21.345134530175198\n",
      "Epoch 88, Loss: 21.301550942015766\n",
      "Epoch 89, Loss: 21.258523466928928\n",
      "Epoch 90, Loss: 21.21603727205921\n",
      "Epoch 91, Loss: 21.17407878641852\n",
      "Epoch 92, Loss: 21.132635428597872\n",
      "Epoch 93, Loss: 21.09169541347834\n",
      "Epoch 94, Loss: 21.051247612303513\n",
      "Epoch 95, Loss: 21.0112814494036\n",
      "Epoch 96, Loss: 20.971786824518514\n",
      "Epoch 97, Loss: 20.932754053296783\n",
      "Epoch 98, Loss: 20.894173820907643\n",
      "Epoch 99, Loss: 20.856037145260377\n",
      "Epoch 100, Loss: 20.818335347365213\n",
      "Epoch 101, Loss: 20.78106002707653\n",
      "Epoch 102, Loss: 20.74420304294497\n",
      "Epoch 103, Loss: 20.707756495244645\n",
      "Epoch 104, Loss: 20.672023404163756\n",
      "Epoch 105, Loss: 20.637527232870713\n",
      "Epoch 106, Loss: 20.60402180426315\n",
      "Epoch 107, Loss: 20.571529814387077\n",
      "Epoch 108, Loss: 20.53983017752875\n",
      "Epoch 109, Loss: 20.50921912258417\n",
      "Epoch 110, Loss: 20.480262884153607\n",
      "Epoch 111, Loss: 20.45293767617773\n",
      "Epoch 112, Loss: 20.426725147209442\n",
      "Epoch 113, Loss: 20.40125844747344\n",
      "Epoch 114, Loss: 20.376322331632498\n",
      "Epoch 115, Loss: 20.351786677817923\n",
      "Epoch 116, Loss: 20.327570918566096\n",
      "Epoch 117, Loss: 20.30362412101097\n",
      "Epoch 118, Loss: 20.279932987447843\n",
      "Epoch 119, Loss: 20.256496259179528\n",
      "Epoch 120, Loss: 20.233280816334283\n",
      "Epoch 121, Loss: 20.210266404018316\n",
      "Epoch 122, Loss: 20.187450171134188\n",
      "Epoch 123, Loss: 20.164837924273378\n",
      "Epoch 124, Loss: 20.142419340147576\n",
      "Epoch 125, Loss: 20.120180736619695\n",
      "Epoch 126, Loss: 20.098113544257487\n",
      "Epoch 127, Loss: 20.076212066631342\n",
      "Epoch 128, Loss: 20.054472241011787\n",
      "Epoch 129, Loss: 20.03289094449224\n",
      "Epoch 130, Loss: 20.01146560179524\n",
      "Epoch 131, Loss: 19.99019396183442\n",
      "Epoch 132, Loss: 19.969073969583366\n",
      "Epoch 133, Loss: 19.94810369223213\n",
      "Epoch 134, Loss: 19.92728127650603\n",
      "Epoch 135, Loss: 19.906604923993726\n",
      "Epoch 136, Loss: 19.886072876938815\n",
      "Epoch 137, Loss: 19.865683410131794\n",
      "Epoch 138, Loss: 19.845434826359483\n",
      "Epoch 139, Loss: 19.825325453921064\n",
      "Epoch 140, Loss: 19.80535364533147\n",
      "Epoch 141, Loss: 19.7855177766925\n",
      "Epoch 142, Loss: 19.765816247424674\n",
      "Epoch 143, Loss: 19.746247480178845\n",
      "Epoch 144, Loss: 19.726809920822884\n",
      "Epoch 145, Loss: 19.707502038443504\n",
      "Epoch 146, Loss: 19.68832232533091\n",
      "Epoch 147, Loss: 19.669269296929752\n",
      "Epoch 148, Loss: 19.650341491749902\n",
      "Epoch 149, Loss: 19.631537471235696\n",
      "Epoch 150, Loss: 19.612855819595524\n",
      "Epoch 151, Loss: 19.594295143595172\n",
      "Epoch 152, Loss: 19.575854072319274\n",
      "Epoch 153, Loss: 19.557531256904895\n",
      "Epoch 154, Loss: 19.539325370251795\n",
      "Epoch 155, Loss: 19.52123510671317\n",
      "Epoch 156, Loss: 19.503259181770524\n",
      "Epoch 157, Loss: 19.485396331695902\n",
      "Epoch 158, Loss: 19.4676453132046\n",
      "Epoch 159, Loss: 19.450004903100485\n",
      "Epoch 160, Loss: 19.432473897916577\n",
      "Epoch 161, Loss: 19.415051113552536\n",
      "Epoch 162, Loss: 19.397735384910884\n",
      "Epoch 163, Loss: 19.380525565533286\n",
      "Epoch 164, Loss: 19.363420527238194\n",
      "Epoch 165, Loss: 19.346419159760913\n",
      "Epoch 166, Loss: 19.329520370396875\n",
      "Epoch 167, Loss: 19.312723083648955\n",
      "Epoch 168, Loss: 19.29602624087941\n",
      "Epoch 169, Loss: 19.279428799966944\n",
      "Epoch 170, Loss: 19.262929734969322\n",
      "Epoch 171, Loss: 19.246528035791858\n",
      "Epoch 172, Loss: 19.230222707862097\n",
      "Epoch 173, Loss: 19.214012771810737\n",
      "Epoch 174, Loss: 19.19789726315909\n",
      "Epoch 175, Loss: 19.181875232013134\n",
      "Epoch 176, Loss: 19.165945742764208\n",
      "Epoch 177, Loss: 19.150107873796234\n",
      "Epoch 178, Loss: 19.134360717199787\n",
      "Epoch 179, Loss: 19.11870337849265\n",
      "Epoch 180, Loss: 19.10313497634694\n",
      "Epoch 181, Loss: 19.08765464232279\n",
      "Epoch 182, Loss: 19.072261520608382\n",
      "Epoch 183, Loss: 19.056954767766317\n",
      "Epoch 184, Loss: 19.041733552486157\n",
      "Epoch 185, Loss: 19.02659705534311\n",
      "Epoch 186, Loss: 19.011544468562594\n",
      "Epoch 187, Loss: 18.996574995790702\n",
      "Epoch 188, Loss: 18.981687851870337\n",
      "Epoch 189, Loss: 18.96688226262293\n",
      "Epoch 190, Loss: 18.95215746463562\n",
      "Epoch 191, Loss: 18.937512705053603\n",
      "Epoch 192, Loss: 18.922947241377855\n",
      "Epoch 193, Loss: 18.90846034126769\n",
      "Epoch 194, Loss: 18.894051282348354\n",
      "Epoch 195, Loss: 18.879719352023223\n",
      "Epoch 196, Loss: 18.865463847290794\n",
      "Epoch 197, Loss: 18.85128407456609\n",
      "Epoch 198, Loss: 18.837179349506425\n",
      "Epoch 199, Loss: 18.82314899684146\n",
      "Epoch 200, Loss: 18.809192350207383\n",
      "Epoch 201, Loss: 18.795308751985083\n",
      "Epoch 202, Loss: 18.781497553142227\n",
      "Epoch 203, Loss: 18.767758113079108\n",
      "Epoch 204, Loss: 18.754089799478177\n",
      "Epoch 205, Loss: 18.740491988157167\n",
      "Epoch 206, Loss: 18.726964062925635\n",
      "Epoch 207, Loss: 18.713505415444896\n",
      "Epoch 208, Loss: 18.70011544509118\n",
      "Epoch 209, Loss: 18.68679355882205\n",
      "Epoch 210, Loss: 18.673539171045796\n",
      "Epoch 211, Loss: 18.660351703493895\n",
      "Epoch 212, Loss: 18.647230585096306\n",
      "Epoch 213, Loss: 18.634175251859684\n",
      "Epoch 214, Loss: 18.621185146748193\n",
      "Epoch 215, Loss: 18.608259719567148\n",
      "Epoch 216, Loss: 18.595398426849105\n",
      "Epoch 217, Loss: 18.582600731742524\n",
      "Epoch 218, Loss: 18.569866103902875\n",
      "Epoch 219, Loss: 18.557194019386095\n",
      "Epoch 220, Loss: 18.544928167607257\n",
      "Epoch 221, Loss: 18.532703775947432\n",
      "Epoch 222, Loss: 18.520410889688403\n",
      "Epoch 223, Loss: 18.508442604136075\n",
      "Epoch 224, Loss: 18.496453162908498\n",
      "Epoch 225, Loss: 18.48436850308425\n",
      "Epoch 226, Loss: 18.472583138994484\n",
      "Epoch 227, Loss: 18.460768285807312\n",
      "Epoch 228, Loss: 18.449194421171548\n",
      "Epoch 229, Loss: 18.437539840033896\n",
      "Epoch 230, Loss: 18.426103665190126\n",
      "Epoch 231, Loss: 18.414569572764805\n",
      "Epoch 232, Loss: 18.4032425225655\n",
      "Epoch 233, Loss: 18.39181435967585\n",
      "Epoch 234, Loss: 18.380585173613987\n",
      "Epoch 235, Loss: 18.36925767832767\n",
      "Epoch 236, Loss: 18.358121816716555\n",
      "Epoch 237, Loss: 18.347231896857643\n",
      "Epoch 238, Loss: 18.33615387158185\n",
      "Epoch 239, Loss: 18.32523113872594\n",
      "Epoch 240, Loss: 18.31451862921924\n",
      "Epoch 241, Loss: 18.303600846618867\n",
      "Epoch 242, Loss: 18.292825968326056\n",
      "Epoch 243, Loss: 18.282251601371296\n",
      "Epoch 244, Loss: 18.271813593391173\n",
      "Epoch 245, Loss: 18.261127055255987\n",
      "Epoch 246, Loss: 18.25056253929825\n",
      "Epoch 247, Loss: 18.240178084862826\n",
      "Epoch 248, Loss: 18.22991794903039\n",
      "Epoch 249, Loss: 18.219748287291868\n",
      "Epoch 250, Loss: 18.209307822357765\n",
      "Epoch 251, Loss: 18.198972625881375\n",
      "Epoch 252, Loss: 18.188803462562465\n",
      "Epoch 253, Loss: 18.17875035854718\n",
      "Epoch 254, Loss: 18.1687827952906\n",
      "Epoch 255, Loss: 18.159297930776937\n",
      "Epoch 256, Loss: 18.149586177559424\n"
     ]
    }
   ],
   "source": [
    "backtracking_theta = backtraking_softmax(X_train, y_train, eta=1, m=0.5, alpha=0.5, batch_size=1024, epochs=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a03d6c-ff9a-4b37-afef-c2133557a6b6",
   "metadata": {},
   "source": [
    "### Prediction on backtracking step-size model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b65e3fc5-7b91-446c-a427-187cf3700494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for the first 30 data points in the training set with the backtracking algorithm:\n",
      " [21 11  8  2 21 16  9  3  7 10  9  4  3  2 21 18 24 11 23 19 24 16 12 16\n",
      " 20 21 13 21 17 13]\n",
      "Actual classes for the first 30 data points in the training set with the backtracking algorithm:\n",
      " [21 11  8  2 21 10  9  3  7  6  9  2  3  6 21  0 24 11 23 19 24  6 12 16\n",
      " 20 21 20 24 23 13]\n",
      "Accuracy for the training set with the backtracking algorithm: 72.18%\n",
      "Weighted Precision on the training set with the backtracking algorithm: 0.72\n",
      "Weighted Recall on the training set with the backtracking algorithm: 0.72\n",
      "Weighted F1 score on the training set with the backtracking algorithm: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Predict on the training set with the backtracking algorithm\n",
    "Y_train_pred = predict(backtracking_theta, X_train)\n",
    "print(\"Predicted classes for the first 30 data points in the training set with the backtracking algorithm:\\n\", Y_train_pred[:30])\n",
    "print(\"Actual classes for the first 30 data points in the training set with the backtracking algorithm:\\n\", y_train[:30])\n",
    "\n",
    "acc_train = accuracy_score(y_train.T, Y_train_pred)\n",
    "print(f\"Accuracy for the training set with the backtracking algorithm: {acc_train:.2f}%\")\n",
    "\n",
    "# Calculate Precision, Recall, and F1 score on the training set with the backtracking algorithm\n",
    "precision, recall, f1 = calculate_f1_score(y_train, Y_train_pred)\n",
    "\n",
    "print(f'Weighted Precision on the training set with the backtracking algorithm: {precision:.2f}')\n",
    "print(f'Weighted Recall on the training set with the backtracking algorithm: {recall:.2f}')\n",
    "print(f'Weighted F1 score on the training set with the backtracking algorithm: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a067bc99-f78a-4d93-b5f7-1a5b2fe27cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for the first 20 data points in the test set with the backtracking algorithm:\n",
      " [14 17  5  5 19 19  1  5  2 14 21  8  7 17 11  1  5 23  5  4]\n",
      "Actual classes for the first 20 data points in the test set with the backtracking algorithm:\n",
      " [16 17  5  5 19 19  6 25  2 14 21  8  7  3  4 16 15 23  5  4]\n",
      "Accuracy for the test set with the backtracking algorithm: 71.98%\n",
      "Weighted Precision on the test set with the backtracking algorithm: 0.72\n",
      "Weighted Recall on the test set with the backtracking algorithm: 0.72\n",
      "Weighted F1 score on the test set with the backtracking algorithm: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set with the backtracking algorithm\n",
    "Y_test_pred = predict(backtracking_theta, X_test)\n",
    "print(\"Predicted classes for the first 20 data points in the test set with the backtracking algorithm:\\n\", Y_test_pred[:20])\n",
    "print(\"Actual classes for the first 20 data points in the test set with the backtracking algorithm:\\n\", y_test[:20])\n",
    "acc_test = accuracy_score(y_test.T, Y_test_pred)\n",
    "print(f\"Accuracy for the test set with the backtracking algorithm: {acc_test:.2f}%\")\n",
    "\n",
    "# Calculate Precision, Recall, and F1 score on the test set with the backtracking algorithm\n",
    "test_precision, test_recall, test_f1 = calculate_f1_score(y_test, Y_test_pred)\n",
    "\n",
    "print(f'Weighted Precision on the test set with the backtracking algorithm: {test_precision:.2f}')\n",
    "print(f'Weighted Recall on the test set with the backtracking algorithm: {test_recall:.2f}')\n",
    "print(f'Weighted F1 score on the test set with the backtracking algorithm: {test_f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6578f390-7349-44dc-bf54-116a2c3310de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for the first 20 data points in the validation set with the backtracking algorithm:\n",
      " [13  5  2 17  9 11  0  1 17 25 18  1 15 11 13 17  5 12  4 19]\n",
      "Actual classes for the first 20 data points in the validation set with the backtracking algorithm:\n",
      " [13  5  6 17  9 11  0  1 18 25 18  1 15 11 13 17  5 12  4 19]\n",
      "Accuracy for the validation set with the backtracking algorithm: 71.89%\n",
      "Weighted Precision on the validation set with the backtracking algorithm: 0.72\n",
      "Weighted Recall on the validation set with the backtracking algorithm: 0.72\n",
      "Weighted F1 score on the validation set with the backtracking algorithm: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set with the backtracking algorithm\n",
    "Y_val_pred = predict(backtracking_theta, X_val)\n",
    "print(\"Predicted classes for the first 20 data points in the validation set with the backtracking algorithm:\\n\", Y_val_pred[:20])\n",
    "print(\"Actual classes for the first 20 data points in the validation set with the backtracking algorithm:\\n\", y_val[:20])\n",
    "\n",
    "acc_val = accuracy_score(y_val.T, Y_val_pred)\n",
    "print(f\"Accuracy for the validation set with the backtracking algorithm: {acc_val:.2f}%\")\n",
    "\n",
    "# Calculate Precision, Recall, and F1 score on the validation set with the backtracking algorithm\n",
    "val_precision, val_recall, val_f1 = calculate_f1_score(y_val, Y_val_pred)\n",
    "\n",
    "print(f'Weighted Precision on the validation set with the backtracking algorithm: {val_precision:.2f}')\n",
    "print(f'Weighted Recall on the validation set with the backtracking algorithm: {val_recall:.2f}')\n",
    "print(f'Weighted F1 score on the validation set with the backtracking algorithm: {val_f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f23231-61b0-4dc6-9f2c-7384540c604e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
